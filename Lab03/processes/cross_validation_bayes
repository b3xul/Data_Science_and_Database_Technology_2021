<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<process version="5.1.011">
  <context>
    <input/>
    <output/>
    <macros/>
  </context>
  <operator activated="true" class="process" compatibility="5.1.011" expanded="true" name="Root">
    <description>&lt;p&gt; In many cases not the learned model is of interest but the accuracy of the model. One possible solution to estimate the predictiveness of the learned model is to apply it to labeled test data and calculate the number of prediction errors (or other performance criteria). Since labeled data is rare, other approaches to estimate the performance of a learning scheme are often used. This process demonstrates &amp;quot;cross validation&amp;quot; in RapidMiner.&lt;/p&gt;  &lt;table&gt; &lt;tr&gt; &lt;td&gt;&lt;icon&gt;groups/24/validation&lt;/icon&gt;&lt;/td&gt; &lt;td&gt;&lt;p&gt;Cross validation divides the labelled data in training and test sets. Models are learned on training data and applied on test data. The prediction errors are calculated and averaged for all subsets. This building block can be used as inner operator for several wrappers like feature generation / selection operators. &lt;/p&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;    &lt;p&gt; This is the first example of a more complex process. The operators build a tree structure. For now it is enough to accept that the cross validation operator demands an example set as input and delivers a vector of performance values as output. Additionally it manages the division into training and test examples. The training examples are used as input for the training learner which delivers a model. This model and the test examples form the input of the applier chain which delivers the performance for this test set. The results for all possible test sets are collected by the cross validation operator. Finally the average is built and delivered as result. &lt;/p&gt;     &lt;p&gt;One of the hardest things for the RapidMiner beginner is often to get an idea of the &lt;b&gt;data flow&lt;/b&gt;. The solution is surprisingly simple: the data flow resembles a depth-first-search through the tree structure. For example, after processing the training set with the first child of the cross validation the learned model, is delivered to the second child (the applier chain). This basic data flow idea is always the same for all processes and thinking in this flow will become very convenient for the experienced user.&lt;/p&gt; &lt;p&gt;Try the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Start the process. The result is a performance estimation of the learning scheme on the input data.&lt;/li&gt;  &lt;li&gt;Select the Evaluation operator and select other performance criteria. The main criterion is used for performance comparisons, for example in a wrapper.&lt;/li&gt;  &lt;li&gt;Replace the cross validation &amp;quot;XVal&amp;quot; by other evaluation schemes and run the process with them. Alternatively you can check how other learners perform on this data and replace the Training operator.&lt;/li&gt;&lt;/ul&gt;</description>
    <parameter key="logverbosity" value="3"/>
    <parameter key="random_seed" value="2001"/>
    <parameter key="send_mail" value="1"/>
    <parameter key="notification_email" value=""/>
    <parameter key="process_duration_for_mail" value="30"/>
    <parameter key="encoding" value="SYSTEM"/>
    <parameter key="parallelize_main_process" value="false"/>
    <process expanded="true" height="414" width="413">
      <operator activated="true" class="retrieve" compatibility="5.1.011" expanded="true" height="60" name="Retrieve" width="90" x="45" y="30">
        <parameter key="repository_entry" value="//Samples/data/Iris"/>
      </operator>
      <operator activated="true" class="x_validation" compatibility="5.1.011" expanded="true" height="112" name="XVal" width="90" x="313" y="75">
        <parameter key="create_complete_model" value="false"/>
        <parameter key="average_performances_only" value="true"/>
        <parameter key="leave_one_out" value="false"/>
        <parameter key="number_of_validations" value="10"/>
        <parameter key="sampling_type" value="stratified sampling"/>
        <parameter key="use_local_random_seed" value="false"/>
        <parameter key="local_random_seed" value="1992"/>
        <parameter key="parallelize_training" value="false"/>
        <parameter key="parallelize_testing" value="false"/>
        <process expanded="true" height="396" width="235">
          <operator activated="true" class="naive_bayes" compatibility="5.1.011" expanded="true" height="76" name="Naive Bayes" width="90" x="134" y="30">
            <parameter key="laplace_correction" value="true"/>
          </operator>
          <connect from_port="training" to_op="Naive Bayes" to_port="training set"/>
          <connect from_op="Naive Bayes" from_port="model" to_port="model"/>
          <portSpacing port="source_training" spacing="0"/>
          <portSpacing port="sink_model" spacing="0"/>
          <portSpacing port="sink_through 1" spacing="0"/>
        </process>
        <process expanded="true" height="396" width="442">
          <operator activated="true" class="apply_model" compatibility="5.1.011" expanded="true" height="76" name="Test" width="90" x="112" y="30">
            <list key="application_parameters"/>
            <parameter key="create_view" value="false"/>
          </operator>
          <operator activated="true" class="performance_classification" compatibility="5.1.011" expanded="true" height="76" name="Performance" width="90" x="342" y="30">
            <parameter key="main_criterion" value="first"/>
            <parameter key="accuracy" value="true"/>
            <parameter key="classification_error" value="false"/>
            <parameter key="kappa" value="false"/>
            <parameter key="weighted_mean_recall" value="false"/>
            <parameter key="weighted_mean_precision" value="false"/>
            <parameter key="spearman_rho" value="false"/>
            <parameter key="kendall_tau" value="false"/>
            <parameter key="absolute_error" value="false"/>
            <parameter key="relative_error" value="false"/>
            <parameter key="relative_error_lenient" value="false"/>
            <parameter key="relative_error_strict" value="false"/>
            <parameter key="normalized_absolute_error" value="false"/>
            <parameter key="root_mean_squared_error" value="false"/>
            <parameter key="root_relative_squared_error" value="false"/>
            <parameter key="squared_error" value="false"/>
            <parameter key="correlation" value="false"/>
            <parameter key="squared_correlation" value="false"/>
            <parameter key="cross-entropy" value="false"/>
            <parameter key="margin" value="false"/>
            <parameter key="soft_margin_loss" value="false"/>
            <parameter key="logistic_loss" value="false"/>
            <parameter key="skip_undefined_labels" value="true"/>
            <parameter key="use_example_weights" value="true"/>
            <list key="class_weights"/>
          </operator>
          <connect from_port="model" to_op="Test" to_port="model"/>
          <connect from_port="test set" to_op="Test" to_port="unlabelled data"/>
          <connect from_op="Test" from_port="labelled data" to_op="Performance" to_port="labelled data"/>
          <connect from_op="Performance" from_port="performance" to_port="averagable 1"/>
          <portSpacing port="source_model" spacing="0"/>
          <portSpacing port="source_test set" spacing="0"/>
          <portSpacing port="source_through 1" spacing="0"/>
          <portSpacing port="sink_averagable 1" spacing="0"/>
          <portSpacing port="sink_averagable 2" spacing="0"/>
        </process>
      </operator>
      <connect from_op="Retrieve" from_port="output" to_op="XVal" to_port="training"/>
      <connect from_op="XVal" from_port="averagable 1" to_port="result 1"/>
      <portSpacing port="source_input 1" spacing="0"/>
      <portSpacing port="sink_result 1" spacing="0"/>
      <portSpacing port="sink_result 2" spacing="0"/>
    </process>
  </operator>
</process>
